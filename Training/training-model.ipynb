{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10753874,"sourceType":"datasetVersion","datasetId":6669766}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T04:31:32.716083Z","iopub.execute_input":"2025-02-15T04:31:32.716350Z","iopub.status.idle":"2025-02-15T04:32:14.015092Z","shell.execute_reply.started":"2025-02-15T04:31:32.716328Z","shell.execute_reply":"2025-02-15T04:32:14.013680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First cell - Imports and Setup\nimport torch\nimport os\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom tqdm.notebook import tqdm  # Use tqdm.notebook for Kaggle\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom PIL import Image\n\n# Check CUDA availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Dataset path for Kaggle\nDATASET_PATH = \"../input/dataset-frame/Dataset_frame\"  # Adjust based on your dataset path\nprint(f\"Dataset path exists: {os.path.exists(DATASET_PATH)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:35:06.466185Z","iopub.execute_input":"2025-02-15T06:35:06.466469Z","iopub.status.idle":"2025-02-15T06:35:13.411721Z","shell.execute_reply.started":"2025-02-15T06:35:06.466432Z","shell.execute_reply":"2025-02-15T06:35:13.411008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Second cell - Dataset and Model Classes\nclass DeepfakeDataset(Dataset):\n    def __init__(self, root_dir, split='Train', transform=None):\n        \"\"\"\n        Args:\n            root_dir: Base directory of the dataset\n            split: 'Train', 'Validation', or 'Test'\n            transform: Optional transforms\n        \"\"\"\n        self.root_dir = os.path.join(root_dir, split)\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        \n        # Verify directory structure\n        print(f\"Loading {split} data from: {self.root_dir}\")\n        \n        # Load both Real and Fake images\n        for label, class_name in enumerate(['Real', 'Fake']):\n            class_dir = os.path.join(self.root_dir, class_name)\n            if not os.path.exists(class_dir):\n                raise RuntimeError(f\"Directory not found: {class_dir}\")\n            \n            files = os.listdir(class_dir)\n            print(f\"Found {len(files)} {class_name} images\")\n            \n            for img_name in files:\n                self.images.append(os.path.join(class_dir, img_name))\n                self.labels.append(label)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        try:\n            image = Image.open(img_path).convert('RGB')\n            label = self.labels[idx]\n            \n            if self.transform:\n                image = self.transform(image)\n                \n            return image, torch.tensor(label, dtype=torch.float32)\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {str(e)}\")\n            # Return a default tensor in case of error\n            return torch.zeros((3, 224, 224)), torch.tensor(0., dtype=torch.float32)\n\ndef get_transforms(is_training=True):\n    if is_training:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                              std=[0.229, 0.224, 0.225])\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                              std=[0.229, 0.224, 0.225])\n        ])\n\nclass DeepfakeDetector(nn.Module):\n    def __init__(self):\n        super(DeepfakeDetector, self).__init__()\n        # Use EfficientNet-B3 with pretrained weights\n        self.base_model = models.efficientnet_b3(weights='IMAGENET1K_V1')\n        num_features = self.base_model.classifier[1].in_features\n        self.base_model.classifier = nn.Identity()\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.3),\n            nn.Linear(num_features, 512),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n            nn.Linear(512, 1)\n        )\n    \n    def forward(self, x):\n        features = self.base_model(x)\n        return self.classifier(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:35:13.412741Z","iopub.execute_input":"2025-02-15T06:35:13.413170Z","iopub.status.idle":"2025-02-15T06:35:13.424297Z","shell.execute_reply.started":"2025-02-15T06:35:13.413145Z","shell.execute_reply":"2025-02-15T06:35:13.423607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(config):\n    # Initialize model and move to GPU\n    model = DeepfakeDetector().to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='max', \n        factor=0.1, \n        patience=3\n    )\n    \n    # Initialize datasets\n    try:\n        train_dataset = DeepfakeDataset(\n            config['data_dir'], \n            \"Train\", \n            transform=get_transforms(True)\n        )\n        val_dataset = DeepfakeDataset(\n            config['data_dir'], \n            \"Validation\", \n            transform=get_transforms(False)\n        )\n    except Exception as e:\n        print(f\"Error initializing datasets: {str(e)}\")\n        return\n\n    # Create data loaders with updated num_workers\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n        persistent_workers=True  # Added for better performance\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n        persistent_workers=True  # Added for better performance\n    )\n\n    # Initialize training variables\n    best_auc = 0.0\n    early_stopping_counter = 0\n    \n    # Updated GradScaler initialization\n    scaler = torch.amp.GradScaler('cuda')\n\n    # Training loop\n    for epoch in range(config['num_epochs']):\n        # Training phase\n        model.train()\n        train_losses = []\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]}')\n        \n        for images, labels in progress_bar:\n            images = images.to(device, non_blocking=True)  # Added non_blocking=True\n            labels = labels.to(device, non_blocking=True)  # Added non_blocking=True\n            \n            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n            \n            # Updated autocast implementation\n            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n                outputs = model(images).squeeze()\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_losses.append(loss.item())\n            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        # Validation phase\n        model.eval()\n        val_losses = []\n        val_preds = []\n        val_labels = []\n        \n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc='Validation'):\n                images = images.to(device, non_blocking=True)\n                labels = labels.to(device, non_blocking=True)\n                \n                # Updated autocast implementation\n                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n                    outputs = model(images).squeeze()\n                    loss = criterion(outputs, labels)\n                \n                val_losses.append(loss.item())\n                val_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n\n        # Calculate metrics\n        val_auc = roc_auc_score(val_labels, val_preds)\n        val_f1 = f1_score(val_labels, np.array(val_preds) > 0.5, average='weighted')\n        \n        print(f'\\nEpoch {epoch+1}:')\n        print(f'Train Loss: {np.mean(train_losses):.4f}')\n        print(f'Val Loss: {np.mean(val_losses):.4f}')\n        print(f'Val AUC: {val_auc:.4f}')\n        print(f'Val F1: {val_f1:.4f}')\n\n        # Update learning rate\n        scheduler.step(val_auc)\n\n        # Save best model\n        if val_auc > best_auc:\n            best_auc = val_auc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'best_auc': best_auc,\n                'scaler_state_dict': scaler.state_dict()\n            }, '/kaggle/working/best_model.pth')\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n\n        # Early stopping\n        if early_stopping_counter >= config['early_stopping_patience']:\n            print(f'\\nEarly stopping triggered after epoch {epoch+1}')\n            break\n\n    print(f'\\nBest validation AUC: {best_auc:.4f}')\n    return model\n\n# Update the configuration with optimized parameters\nconfig = {\n    'data_dir': DATASET_PATH,\n    'batch_size': 64,  # You can try 64 if memory allows\n    'learning_rate': 1e-4,\n    'num_epochs': 20,\n    'early_stopping_patience': 5\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:35:54.281364Z","iopub.execute_input":"2025-02-15T06:35:54.281684Z","iopub.status.idle":"2025-02-15T06:35:54.305243Z","shell.execute_reply.started":"2025-02-15T06:35:54.281659Z","shell.execute_reply":"2025-02-15T06:35:54.304328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add memory optimization before training\ntorch.backends.cudnn.benchmark = True  # Enable cudnn autotuner\ntorch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere\ntorch.backends.cudnn.allow_tf32 = True  # Allow TF32 on Ampere\n\n# Train the model\nmodel = train_model(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:35:57.968187Z","iopub.execute_input":"2025-02-15T06:35:57.968483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model_path, data_dir):\n    test_dataset = DeepfakeDataset(data_dir, \"Test\", transform=get_transforms(False))\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\n    model = DeepfakeDetector().to(device)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n\n    test_preds, test_labels = [], []\n\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n            images = images.to(device)\n            outputs = model(images).squeeze()\n            test_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n            test_labels.extend(labels.numpy())\n\n    auc_score = roc_auc_score(test_labels, test_preds)\n    print(f\"Test AUC-ROC Score: {auc_score:.4f}\")\n\nif __name__ == \"__main__\":\n    test_model(\"/kaggle/working/best_model.pth\", DATASET_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}