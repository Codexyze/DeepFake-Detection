{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:07:59.227819Z",
     "iopub.status.busy": "2025-02-17T19:07:59.227460Z",
     "iopub.status.idle": "2025-02-17T19:08:05.721000Z",
     "shell.execute_reply": "2025-02-17T19:08:05.720264Z",
     "shell.execute_reply.started": "2025-02-17T19:07:59.227793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from PIL import Image\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "DATASET_PATH = \"/kaggle/input/dataset-frame-bigger/Dataframe\"  \n",
    "print(f\"Dataset path exists: {os.path.exists(DATASET_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:08:07.376970Z",
     "iopub.status.busy": "2025-02-17T19:08:07.376562Z",
     "iopub.status.idle": "2025-02-17T19:08:07.387430Z",
     "shell.execute_reply": "2025-02-17T19:08:07.386615Z",
     "shell.execute_reply.started": "2025-02-17T19:08:07.376948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='Train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Base directory of the dataset\n",
    "            split: 'Train', 'Validation', or 'Test'\n",
    "            transform: Optional transforms\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"Loading {split} data from: {self.root_dir}\")\n",
    "        \n",
    "        for label, class_name in enumerate(['Real', 'Fake']):\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                raise RuntimeError(f\"Directory not found: {class_dir}\")\n",
    "            \n",
    "            files = os.listdir(class_dir)\n",
    "            print(f\"Found {len(files)} {class_name} images\")\n",
    "            \n",
    "            for img_name in files:\n",
    "                self.images.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            return torch.zeros((3, 224, 224)), torch.tensor(0., dtype=torch.float32)\n",
    "\n",
    "def get_transforms(is_training=True):\n",
    "    if is_training:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepfakeDetector, self).__init__()\n",
    "        self.base_model = models.efficientnet_b4(weights='IMAGENET1K_V1')\n",
    "        num_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:08:09.987003Z",
     "iopub.status.busy": "2025-02-17T19:08:09.986700Z",
     "iopub.status.idle": "2025-02-17T19:08:10.009617Z",
     "shell.execute_reply": "2025-02-17T19:08:10.008845Z",
     "shell.execute_reply.started": "2025-02-17T19:08:09.986979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    model = DeepfakeDetector().to(device, memory_format=torch.channels_last)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=3e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    try:\n",
    "        train_dataset = DeepfakeDataset(\n",
    "            config['data_dir'], \n",
    "            \"Train\", \n",
    "            transform=get_transforms(True)\n",
    "        )\n",
    "        val_dataset = DeepfakeDataset(\n",
    "            config['data_dir'], \n",
    "            \"Val\", \n",
    "            transform=get_transforms(False)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing datasets: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    best_auc = 0.0\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]}')\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
    "            labels = labels.to(device, non_blocking=True)  \n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True) \n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(images).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc='Validation'):\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(images).squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                val_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, np.array(val_preds) > 0.5, average='weighted')\n",
    "        val_acc = accuracy_score(val_labels, (np.array(val_preds) > 0.5)) \n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Train Loss: {np.mean(train_losses):.4f}')\n",
    "        print(f'Val Loss: {np.mean(val_losses):.4f}')\n",
    "        print(f'Val AUC: {val_auc:.4f}')\n",
    "        print(f'Val F1: {val_f1:.4f}')\n",
    "        print(f'Val Accuracy: {val_acc:.4f}')  \n",
    "\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            save_path = f\"/kaggle/working/best_b3_model_epoch{epoch+1}.pth\"\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_auc': best_auc,\n",
    "                'scaler_state_dict': scaler.state_dict()\n",
    "            }, save_path)\n",
    "            \n",
    "           \n",
    "            !cp {save_path} /kaggle/working/best_model.pth\n",
    "            !cp {save_path} /kaggle/output/  \n",
    "            print(f\"✅ Model saved at: {save_path} & /kaggle/output/\")\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= config['early_stopping_patience']:\n",
    "            print(f'\\nEarly stopping triggered after epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f'\\nBest validation AUC: {best_auc:.4f}')\n",
    "    return model\n",
    "\n",
    "config = {\n",
    "    'data_dir': DATASET_PATH,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-5,\n",
    "    'num_epochs': 10,\n",
    "    'early_stopping_patience': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:50:34.364943Z",
     "iopub.status.busy": "2025-02-17T15:50:34.364656Z",
     "iopub.status.idle": "2025-02-17T15:50:57.706953Z",
     "shell.execute_reply": "2025-02-17T15:50:57.705573Z",
     "shell.execute_reply.started": "2025-02-17T15:50:34.364921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True  \n",
    "torch.backends.cuda.matmul.allow_tf32 = True  \n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "model = train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:17:21.480000Z",
     "iopub.status.busy": "2025-02-17T19:17:21.479710Z",
     "iopub.status.idle": "2025-02-17T19:17:21.591541Z",
     "shell.execute_reply": "2025-02-17T19:17:21.590887Z",
     "shell.execute_reply.started": "2025-02-17T19:17:21.479979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil \n",
    "\n",
    "source_path = '/kaggle/input/efficientnet_b4_model/pytorch/default/1/best_b3_model_epoch4.pth' \n",
    "\n",
    "destination_path = \"/kaggle/working/\" \n",
    "\n",
    "shutil.copy(source_path, destination_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:20:36.287400Z",
     "iopub.status.busy": "2025-02-17T19:20:36.287090Z",
     "iopub.status.idle": "2025-02-17T19:20:36.932686Z",
     "shell.execute_reply": "2025-02-17T19:20:36.931788Z",
     "shell.execute_reply.started": "2025-02-17T19:20:36.287374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_path = \"/kaggle/working/best_b3_model_epoch4.pth\"\n",
    "checkpoint = torch.load(model_path, map_location=\"cuda\") \n",
    "\n",
    "model = DeepfakeDetector().to(\"cuda\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model successfully loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:21:44.243436Z",
     "iopub.status.busy": "2025-02-17T19:21:44.243139Z",
     "iopub.status.idle": "2025-02-17T19:24:45.046507Z",
     "shell.execute_reply": "2025-02-17T19:24:45.045563Z",
     "shell.execute_reply.started": "2025-02-17T19:21:44.243413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, data_dir):\n",
    "    test_dataset = DeepfakeDataset(data_dir, \"Test\", transform=get_transforms(False))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    test_preds, test_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images = images.to(\"cuda\")\n",
    "            outputs = model(images).squeeze()\n",
    "            test_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            test_labels.extend(labels.numpy())\n",
    "\n",
    "    auc_score = roc_auc_score(test_labels, test_preds)\n",
    "    print(f\"Test AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "test_model(model, DATASET_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6669766,
     "sourceId": 10753874,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 244712,
     "modelInstanceId": 222951,
     "sourceId": 260789,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 245078,
     "modelInstanceId": 223312,
     "sourceId": 261191,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
